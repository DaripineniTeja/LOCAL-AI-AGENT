# ğŸ• Local AI Pizza Assistant

This is a LangChain-based local AI chatbot that answers user questions about a pizza restaurant using real customer reviews. It runs fully offline using Ollama and features both a terminal CLI and a Gradio web interface.

---

## ğŸ“Œ Features

- ğŸ” Answers questions using retrieved pizza reviews
- ğŸ§  Powered by LLaMA3.2 via Ollama
- ğŸ›œ Runs completely offline on your local machine
- ğŸŒ Gradio GUI for easy interaction
- âš¡ Fast response using LangChain chaining logic

---

## ğŸš€ How It Works

1. The user asks a question (e.g., "Are vegan options available?")
2. The system retrieves relevant reviews
3. LLaMA3 generates a response using the reviews as context

---

## ğŸ› ï¸ Tech Stack

- [LangChain](https://www.langchain.com/)
- [Ollama](https://ollama.com/)
- [Gradio](https://www.gradio.app/)
- Python 3.10+
- Local Model: `llama3.2` (or any Ollama-supported model)

---

## ğŸ“ Project Structure

ğŸ“¦ Local AI Agent/
â”œâ”€â”€ main.py # CLI mode (manual Q&A)
â”œâ”€â”€ app.py # Gradio web app interface
â”œâ”€â”€ vector.py # Review retrieval logic
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Youâ€™re here!
â””â”€â”€ .gitignore # Ignore venv, cache, etc.



---

## ğŸ–¥ï¸ Installation

### ğŸ”§ 1. Clone this repo
```bash
git clone https://github.com/DaripineniTeja/local-ai-pizza-assistant.git
cd local-ai-pizza-assistant

###ğŸ”§ 2. Install dependencies
pip install -r requirements.txt

###ğŸ”§ 3. Install Ollama & Run the model
# Install ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull the model
ollama pull llama3

# Start the Ollama server
ollama serve



âœ¨ Author
Daripineni Teja
ğŸ”— Instagram
ğŸ“§ aiwithteja

